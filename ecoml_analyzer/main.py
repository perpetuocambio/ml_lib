"""
Aplicaci√≥n principal de EcoML Analyzer
Demostraci√≥n de componentes generales de ml_lib aplicados al dominio ecol√≥gico
"""
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import os

# Importar componentes de nuestra biblioteca
from ml_lib.core import LoggingService
from ecoml_analyzer.data import EcologicalDataReader, AbundanceNormalizer, DataFilter
from ecoml_analyzer.analysis import EcologicalAnalyzer, EcologicalModelEstimator
from ecoml_analyzer.visualization import EcologicalVisualizer


def generate_synthetic_ecological_data(
    n_species: int = 50,
    n_sites: int = 20,
    n_habitats: int = 3,
    random_state: int = 42
) -> Tuple[np.ndarray, List[str], List[str], np.ndarray]:
    """
    Genera datos sint√©ticos ecol√≥gicos para demostraci√≥n.
    """
    np.random.seed(random_state)
    
    # Crear nombres de especies y sitios
    species_names = [f"Species_{i:03d}" for i in range(n_species)]
    site_names = [f"Site_{i:02d}" for i in range(n_sites)]
    
    # Generar matriz de abundancia con estructura de h√°bitat
    abundance_matrix = np.zeros((n_species, n_sites))
    
    # Definir especies preferentes para cada h√°bitat
    species_per_habitat = n_species // n_habitats
    for habitat in range(n_habitats):
        start_idx = habitat * species_per_habitat
        end_idx = (habitat + 1) * species_per_habitat if habitat < n_habitats - 1 else n_species
        
        # Asignar rangos de especies a cada h√°bitat
        habitat_species = range(start_idx, end_idx)
        
        # Definir sitios para cada h√°bitat
        sites_per_habitat = n_sites // n_habitats
        habitat_sites_start = habitat * sites_per_habitat
        habitat_sites_end = (habitat + 1) * sites_per_habitat if habitat < n_habitats - 1 else n_sites
        
        # A√±adir abundancia a especies en su h√°bitat
        for site_idx in range(habitat_sites_start, habitat_sites_end):
            for sp_idx in habitat_species:
                # Abundancia base con variaci√≥n
                base_abundance = np.random.poisson(5)
                # A√±adir efecto de h√°bitat
                habitat_effect = 10 if np.random.random() > 0.3 else 5  # Mayor abundancia para especies de h√°bitat
                abundance_matrix[sp_idx, site_idx] = base_abundance + habitat_effect
    
    # A√±adir algo de ruido y ocurrencia cero para realismo
    noise = np.random.poisson(1, size=(n_species, n_sites))
    abundance_matrix = np.maximum(abundance_matrix + noise, 0)
    
    # Generar datos ambientales simples
    environmental_data = np.random.normal(0, 1, (n_sites, 5))  # 5 variables ambientales
    
    return abundance_matrix, species_names, site_names, environmental_data


def main():
    """
    Funci√≥n principal que demuestra componentes generales de ml_lib aplicados al dominio ecol√≥gico.
    """
    print("üåø EcoML Analyzer - Demostraci√≥n de componentes generales de ml_lib")
    print("=" * 65)
    
    # Configurar logging
    logger_service = LoggingService("EcoML_Demo")
    logger = logger_service.get_logger()
    logger.info("Iniciando demostraci√≥n de EcoML Analyzer con componentes generales")
    
    # 1. Generar datos sint√©ticos
    print("\n1. Generando datos ecol√≥gicos sint√©ticos...")
    abundance_matrix, species_names, site_names, environmental_data = generate_synthetic_ecological_data()
    
    print(f"   - Matriz de abundancia: {abundance_matrix.shape}")
    print(f"   - Especies: {len(species_names)}")
    print(f"   - Sitios: {len(site_names)}")
    print(f"   - Datos ambientales: {environmental_data.shape}")
    
    # 2. Preprocesamiento de datos usando componentes generales
    print("\n2. Preprocesando datos con componentes generales...")
    
    # Usar el transformador de normalizaci√≥n que implementa la interfaz general
    normalizer = AbundanceNormalizer(method='sqrt')
    normalized_matrix = normalizer.fit_transform(abundance_matrix)
    print(f"   - Datos normalizados usando transformador general: {normalized_matrix.shape}")
    
    # Usar el filtro general para datos
    filtered_matrix, filtered_species = DataFilter.filter_by_occurrence(
        normalized_matrix, 
        species_names,
        min_occurrence_proportion=0.1,
        min_total_value=1
    )
    print(f"   - Especies despu√©s de filtrado: {len(filtered_matrix)}")
    
    # 3. An√°lisis usando componentes generales aplicados al dominio
    print("\n3. Aplicando t√©cnicas generales de ml_lib al dominio ecol√≥gico...")
    analyzer = EcologicalAnalyzer()
    
    composition_results = analyzer.analyze_composition(filtered_matrix.T)  # Transponer para an√°lisis por sitio
    print(f"   - An√°lisis de composici√≥n completado")
    print(f"   - Riqueza media por sitio: {composition_results['richness'].mean():.2f}")
    print(f"   - Diversidad media: {composition_results['diversity'].mean():.2f}")
    
    # 4. Reducci√≥n de dimensionalidad con componentes generales
    print("\n4. Aplicando reducci√≥n de dimensionalidad general...")
    # Transponer para tener sitios como muestras y especies como caracter√≠sticas
    dim_red_results = analyzer.perform_dimensionality_reduction(
        filtered_matrix.T,  # sitios x especies
        method='custom_pca',
        n_components=5
    )
    
    print(f"   - M√©todo: {dim_red_results['method']}")
    print(f"   - Componentes principales: {dim_red_results['n_components']}")
    print(f"   - Varianza explicada PC1: {dim_red_results['explained_variance_ratio'][0]:.2%}")
    
    # 5. Clustering con componentes generales
    print("\n5. Aplicando clustering general...")
    clustering_results = analyzer.perform_clustering(
        filtered_matrix.T,  # sitios x especies
        n_clusters=3,
        method='kmeans'
    )
    
    print(f"   - M√©todo de clustering: {clustering_results['clustering_method']}")
    print(f"   - N√∫mero de clusters: {clustering_results['n_clusters']}")
    print(f"   - Silhouette Score: {clustering_results['silhouette_score']:.3f}")
    print(f"   - Calinski-Harabasz Score: {clustering_results['calinski_harabasz_score']:.1f}")
    
    # 6. Demostraci√≥n del estimador general aplicado a datos ecol√≥gicos
    print("\n6. Demostrando estimador general aplicado al dominio ecol√≥gico...")
    
    # Simular una tarea de clasificaci√≥n binaria (por ejemplo, presencia/ausencia de una especie)
    # Usar los primeros componentes principales como features
    X_features = dim_red_results['transformed_data'][:, :3]  # Usar primeros 3 componentes
    y_target = (filtered_matrix[0, :] > np.median(filtered_matrix[0, :])).astype(int)  # Clasificaci√≥n basada en abundancia de primera especie
    
    # Crear y entrenar el estimador ecol√≥gico que implementa la interfaz general
    eco_model = EcologicalModelEstimator(model_type='logistic', random_state=42)
    eco_model.fit(X_features, y_target)
    
    # Realizar predicciones
    predictions = eco_model.predict(X_features)
    accuracy = np.mean(predictions == y_target)
    
    print(f"   - Estimador entrenado con {X_features.shape[1]} features")
    print(f"   - Precisi√≥n en entrenamiento: {accuracy:.3f}")
    print(f"   - Demostrando interfaz general EstimatorInterface aplicada a ecolog√≠a")
    
    # 7. Visualizaci√≥n de resultados
    print("\n7. Generando visualizaciones...")
    visualizer = EcologicalVisualizer()
    
    # Heatmap de abundancia
    print("   - Creando heatmap de abundancia...")
    fig1 = visualizer.plot_species_abundance_heatmap(
        filtered_matrix[:15],  # Solo primeras 15 especies para claridad
        filtered_species[:15],
        site_names,
        "Heatmap de Abundancia de Especies (Primeras 15)"
    )
    
    # An√°lisis de composici√≥n
    print("   - Creando plot de an√°lisis de composici√≥n...")
    fig2 = visualizer.plot_composition_analysis(
        {
            'richness': composition_results['richness'],
            'diversity': composition_results['diversity'],
            'total_values': np.sum(filtered_matrix, axis=0),
            'abundance_distribution': composition_results['abundance_distribution']
        },
        site_names,
        "An√°lisis de Composici√≥n Ecol√≥gica"
    )
    
    # An√°lisis de dimensionalidad
    print("   - Creando plot de an√°lisis de dimensionalidad...")
    fig3 = visualizer.plot_dimensionality_reduction(
        dim_red_results,
        site_names,
        "An√°lisis de Dimensionalidad General Aplicado a Ecolog√≠a"
    )
    
    # Resultados de clustering
    print("   - Creando plot de clustering...")
    fig5 = visualizer.plot_clustering_results(
        clustering_results,
        filtered_matrix.T,  # Pasar matriz en formato correcto
        site_names,
        "Clustering General Aplicado a Datos Ecol√≥gicos"
    )
    
    # 8. Guardar visualizaciones
    print("\n8. Guardando visualizaciones...")
    os.makedirs("ecoml_output", exist_ok=True)
    visualizer.save_all_figures("ecoml_output")
    print("   - Visualizaciones guardadas en ecoml_output/")
    
    # 9. Resumen de resultados
    print("\nüìä Resumen de la demostraci√≥n:")
    print(f"   - Especies analizadas: {len(filtered_species)}")
    print(f"   - Sitios analizados: {len(site_names)}")
    print(f"   - Aplicaci√≥n de componentes generales de ml_lib al dominio ecol√≥gico")
    print(f"   - Uso de interfaces generales: TransformerInterface, EstimatorInterface")
    print(f"   - T√©cnicas aplicadas: normalizaci√≥n, filtrado, PCA, clustering, clasificaci√≥n")
    print(f"   - Precisi√≥n del modelo ecol√≥gico: {accuracy:.3f}")
    
    print("\n‚úÖ Demostraci√≥n completada exitosamente!")
    print("   La aplicaci√≥n muestra c√≥mo t√©cnicas generales de ml_lib se aplican al dominio ecol√≥gico")
    print("   Las visualizaciones est√°n disponibles en el directorio 'ecoml_output/'")
    
    # Mostrar que se usaron componentes generales
    print(f"\nüîç Componentes generales de ml_lib demostrados:")
    print(f"   - Interfaz TransformerInterface para transformadores de datos")
    print(f"   - Interfaz EstimatorInterface para modelos predictivos")
    print(f"   - Servicios de validaci√≥n y logging")
    print(f"   - Operaciones de √°lgebra lineal general (SVD para PCA)")
    print(f"   - Aplicaci√≥n agn√≥stica al dominio con enfoque ecol√≥gico")
    
    logger.info("Demostraci√≥n completada exitosamente")
    
    return {
        'abundance_matrix': filtered_matrix,
        'species_names': filtered_species,
        'site_names': site_names,
        'environmental_data': environmental_data,
        'composition_results': composition_results,
        'dim_red_results': dim_red_results,
        'clustering_results': clustering_results,
        'model_results': {
            'accuracy': accuracy,
            'predictions': predictions,
            'true_values': y_target
        }
    }


def run_simple_example():
    """
    Ejecuta un ejemplo simple para probar r√°pidamente la funcionalidad.
    """
    print("üåø Ejemplo simple de EcoML Analyzer")
    print("=" * 40)
    
    # Generar datos peque√±os
    n_species, n_sites = 10, 8
    abundance_matrix = np.random.poisson(3, size=(n_species, n_sites))
    species_names = [f"SP_{i}" for i in range(n_species)]
    site_names = [f"SITE_{i}" for i in range(n_sites)]
    
    # Usar componentes generales aplicados al dominio
    normalizer = AbundanceNormalizer(method='sqrt')
    normalized_matrix = normalizer.fit_transform(abundance_matrix)
    
    analyzer = EcologicalAnalyzer()
    composition_results = analyzer.analyze_composition(normalized_matrix.T)
    
    print(f"Especies analizadas: {n_species}")
    print(f"Sitios analizados: {n_sites}")
    print(f"Riqueza media: {composition_results['richness'].mean():.2f}")
    print(f"Diversidad media: {composition_results['diversity'].mean():.2f}")
    print("¬°Ejemplo simple completado!")
    print("El ejemplo demuestra componentes generales de ml_lib aplicados a datos ecol√≥gicos")


if __name__ == "__main__":
    # Ejecutar ejemplo simple primero
    run_simple_example()
    print("\n" + "="*60 + "\n")
    
    # Ejecutar demostraci√≥n completa
    results = main()