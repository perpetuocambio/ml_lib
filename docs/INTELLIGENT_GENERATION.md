# Intelligent Image Generation - Complete Guide

## Philosophy: Zero Configuration, Maximum Intelligence

**User provides**: `prompt` + simple options
**System handles**: EVERYTHING technical

No need to understand:
- Model architectures (SD1.5, SDXL, Flux, etc.)
- Component compatibility (VAE, encoders, LoRAs)
- Memory optimization strategies
- Generation parameters (steps, CFG, samplers)
- Model selection criteria

## Quick Start

```python
from ml_lib.diffusion.intelligent.pipeline import IntelligentPipelineBuilder

# Auto-detect ComfyUI and generate
builder = IntelligentPipelineBuilder.from_comfyui_auto()
image = builder.generate("a beautiful sunset")
image.save("sunset.png")
```

That's it! The system automatically:
1. ‚úÖ Detects your ComfyUI installation
2. ‚úÖ Indexes all available models (3,678 LoRAs, checkpoints, etc.)
3. ‚úÖ Selects optimal base model for prompt
4. ‚úÖ Chooses compatible VAE and encoders
5. ‚úÖ Applies memory optimization based on GPU
6. ‚úÖ Configures optimal generation parameters
7. ‚úÖ Generates image

## User API

### Simplest Generation

```python
builder = IntelligentPipelineBuilder.from_comfyui_auto()
image = builder.generate("your prompt here")
```

### With Style Hints

```python
image = builder.generate(
    prompt="a girl with pink hair",
    style="anime",  # "realistic", "artistic", "3d", etc.
    quality="high"  # "fast", "balanced", "high", "ultra"
)
```

Style hints help select better models:
- `"realistic"` ‚Üí photorealistic base models
- `"anime"` ‚Üí anime-optimized models
- `"artistic"` ‚Üí painting/art-style models

Quality levels:
- `"fast"` ‚Üí Quick testing (fewer steps)
- `"balanced"` ‚Üí Good quality/speed balance (default)
- `"high"` ‚Üí Better quality (more steps, better models)
- `"ultra"` ‚Üí Maximum quality (may be slow)

### With Ollama Intelligence (RECOMMENDED)

```python
# Enable semantic prompt analysis
builder = IntelligentPipelineBuilder.from_comfyui_auto(enable_ollama=True)

image = builder.generate("cyberpunk city at night with neon lights")
```

With Ollama enabled, the system:
1. **Analyzes prompt semantically** using LLM
2. **Extracts key concepts**: "cyberpunk", "city", "night", "neon"
3. **Selects best base model** for style
4. **Recommends LoRAs** that match concepts
5. **Optimizes parameters** for prompt type

### Multiple Images

```python
images = builder.generate(
    prompt="a fantasy castle",
    num_images=4,  # Generate 4 variations
    seed=42        # Reproducible
)

for i, img in enumerate(images):
    img.save(f"castle_{i}.png")
```

### Custom Model Paths

If you don't use ComfyUI:

```python
builder = IntelligentPipelineBuilder.from_paths(
    model_paths={
        "checkpoint": ["/models/checkpoints"],
        "lora": ["/models/loras"],
        "vae": ["/models/vae"],
    },
    enable_ollama=True
)
```

### Advanced Overrides

For advanced users who want control:

```python
image = builder.generate(
    prompt="a portrait of a wizard",
    style="realistic",
    quality="ultra",
    # Technical overrides
    steps=50,              # Override steps
    cfg_scale=9.0,         # Override guidance
    sampler="DPM++ 2M Karras",  # Specific sampler
    width=1536,            # Custom resolution
    height=2048,
    negative_prompt="blur, low quality, distorted"
)
```

## Architecture

### 1. IntelligentPipelineBuilder

**User-facing API**. Simple interface, handles everything.

```python
builder = IntelligentPipelineBuilder.from_comfyui_auto(enable_ollama=True)
```

**Responsibilities**:
- Auto-detect or accept user model paths
- Initialize ModelOrchestrator
- Manage ResourceMonitor
- Provide simple `generate()` API
- Handle memory optimization
- Clean up resources

### 2. ModelOrchestrator

**Model selection and orchestration**. Invisible to user.

**Responsibilities**:
- Scan and index models with metadata
- Parse `.metadata.json` files (from civitai_comfy_nodes)
- Match models to prompts
- Ensure component compatibility
- Manage architecture requirements

**Key Features**:
- Reads CivitAI metadata for optimal parameters
- Uses popularity scores for selection
- Respects base model compatibility
- Checks memory constraints

### 3. OllamaModelSelector

**Semantic prompt analysis**. Intelligent model selection.

```python
# Invisible to user, used internally
selector = OllamaModelSelector(ollama_model="llama3.2")
analysis = selector.analyze_prompt("cyberpunk city at night")
```

**Analysis Output**:
```python
PromptAnalysis(
    style="realistic",
    content_type="scene",
    suggested_base_model="SDXL",
    key_concepts=["cyberpunk", "city", "night", "neon"],
    recommended_lora_tags=["cyberpunk", "sci-fi", "urban", "neon"],
    suggested_steps=30,
    suggested_cfg=7.0
)
```

**ModelMatcher** then:
1. Scores base models against analysis
2. Scores LoRAs against concepts
3. Returns best matches

### 4. MetadataFetcher

**Secure, anonymous metadata fetching**. Our own implementation.

```python
fetcher = MetadataFetcher()

# Fetch metadata for a model file
metadata = fetcher.fetch_for_file("/models/loras/anime_girl.safetensors")
```

**Features**:
- ‚úÖ **Anonymous**: No API keys, no tracking
- ‚úÖ **Secure**: Safe requests, rate limiting
- ‚úÖ **Cached**: 24h local cache
- ‚úÖ **Our format**: Independent of ComfyUI JSON
- ‚úÖ **Respectful**: 1s min interval between requests

**Uses CivitAI public API**:
```
GET /api/v1/model-versions/by-hash/{SHA256}
```

**Fetches**:
- Model name, base architecture
- Trigger words, tags
- Download count, rating (popularity)
- Optimal parameters from example images
- Description

**Saves to**:
```
~/.ml_lib/metadata_cache/{hash}.json
```

### 5. ResourceMonitor

**Reusable system monitoring**. Independent module.

```python
from ml_lib.system import ResourceMonitor

monitor = ResourceMonitor()
stats = monitor.get_current_stats()

print(f"GPU Memory: {stats.get_primary_gpu().memory_used_gb:.1f}GB")
print(f"GPU Temp: {stats.get_primary_gpu().temperature_celsius}¬∞C")
print(f"CPU Usage: {stats.cpu.usage_percent}%")
```

**Monitors**:
- GPU: memory, utilization, temperature, power, fan
- CPU: usage, temperature, frequency
- RAM: total, used, available

**Features**:
- Thermal throttling detection
- Device recommendations
- Model size estimation

**Reusable**: Can be used in ANY ML project, not just diffusion.

### 6. DiffusionArchitecture

**Architecture definitions**. Invisible to user.

Defines requirements for each architecture:

```python
SDXL = DiffusionArchitecture(
    name=BaseModel.SDXL,
    requires_vae=True,
    requires_text_encoder=True,
    requires_text_encoder_2=True,  # Dual encoders
    typical_base_size_gb=6.5,
    compatible_vae_patterns=["sdxl", "xl-vae"],
    default_steps=30,
    default_cfg=7.0,
    default_sampler="DPM++ 2M SDE Karras"
)
```

Supported architectures:
- **SD1.5**: Stable Diffusion 1.5
- **SDXL**: Stable Diffusion XL
- **Pony**: Pony Diffusion (SDXL-based)
- **SD3**: Stable Diffusion 3
- **Flux**: Flux.1

## Metadata Flow

### 1. ComfyUI Custom Nodes

User has ComfyUI with custom_nodes:
- `civitai_comfy_nodes`: Downloads metadata from CivitAI
- `comfyui-lora-manager`: Manages LoRA metadata

These create `.metadata.json` files:
```
/models/loras/anime_girl.safetensors
/models/loras/anime_girl.metadata.json  ‚Üê Created by custom_node
```

### 2. Our MetadataFetcher (Independent)

We also provide **our own metadata fetcher**:

```python
from ml_lib.diffusion.services import MetadataFetcher

fetcher = MetadataFetcher()

# Method 1: From file (auto-fetches if not cached)
metadata = fetcher.fetch_for_file("/models/loras/model.safetensors")

# Method 2: From hash (if you have SHA256)
metadata = fetcher.fetch_by_hash("af7ed3e1fc3794bb...")

# Method 3: Bulk update directory
results = fetcher.update_directory("/models/loras")
```

**Why our own fetcher?**
- ‚úÖ Independent from ComfyUI
- ‚úÖ Privacy-focused (anonymous)
- ‚úÖ Our own format (not tied to custom_node JSON)
- ‚úÖ Works without ComfyUI
- ‚úÖ Rate limiting, caching built-in

### 3. Metadata Usage

ModelOrchestrator reads metadata from BOTH sources:
1. ComfyUI `.metadata.json` files (if available)
2. Our own cache `~/.ml_lib/metadata_cache/`

Uses metadata for:
- Model selection (popularity, tags)
- Parameter optimization (steps, CFG from examples)
- Trigger word injection
- Architecture detection

## Memory Optimization

Automatic memory optimization based on:
1. **Quality setting** ‚Üí Memory requirement
2. **Available GPU memory** ‚Üí Optimization level

```python
# User chooses quality
quality = "high"  # Needs ~12GB

# System checks GPU
available = 8.5  # GB

# Selects optimization
if available >= 12.0:
    opt_level = NONE
elif available >= 8.0:
    opt_level = BALANCED  # ‚Üê Selected
else:
    opt_level = AGGRESSIVE
```

Optimization techniques (from HuggingFace):
- Sequential CPU Offload
- Model CPU Offload
- Group Offloading
- VAE Tiling / Slicing
- Attention Slicing
- xFormers
- FP8 Layerwise Casting

See `ml_lib/diffusion/intelligent/memory/services/memory_optimizer.py`

## Example: What Happens Behind the Scenes

User code:
```python
builder = IntelligentPipelineBuilder.from_comfyui_auto(enable_ollama=True)
image = builder.generate("anime girl with pink hair", style="anime", quality="high")
```

Behind the scenes:

### 1. Initialization
```
‚úì Detect ComfyUI at /src/ComfyUI
‚úì Scan models: 3,678 LoRAs, 45 checkpoints, 12 VAEs
‚úì Load metadata from .metadata.json files
‚úì Initialize Ollama connection
‚úì Initialize ResourceMonitor
‚úì Check GPU: NVIDIA RTX 4090, 24GB available
```

### 2. Prompt Analysis (Ollama)
```
‚Üí Send to Ollama: "Analyze: 'anime girl with pink hair'"
‚Üê Response: {
    style: "anime",
    content_type: "character",
    key_concepts: ["anime", "girl", "pink", "hair"],
    suggested_base_model: "SDXL",
    recommended_lora_tags: ["anime", "character", "girl", "hair"]
  }
```

### 3. Model Selection
```
‚Üí Score base models for "anime" + SDXL
  ‚úì "AnimeXL_v3" score: 87.5 (high popularity, "anime" tag)
  ‚úì "RealisticVision" score: 45.2 (realistic, not anime)

‚Üí Select: AnimeXL_v3

‚Üí Score LoRAs for ["anime", "girl", "hair"]
  ‚úì "anime_girl_v2" score: 92.3 (tags match, high downloads)
  ‚úì "pink_hair_style" score: 78.1 (specific to pink hair)
  ‚úì "character_detail" score: 65.4 (general character)

‚Üí Select: anime_girl_v2 (weight 0.8), pink_hair_style (weight 0.7)
```

### 4. Component Selection
```
‚Üí Architecture: SDXL (requires VAE + 2 encoders)
‚Üí Select compatible VAE: sdxl_vae.safetensors
‚Üí Encoders: Auto-loaded with base model
```

### 5. Parameter Optimization
```
‚Üí Quality "high" + GPU 24GB ‚Üí Optimization: BALANCED
‚Üí Use metadata optimal params:
  steps: 30 (from model examples)
  cfg: 7.0
  sampler: "DPM++ 2M SDE Karras"
  clip_skip: 2
```

### 6. Generation
```
‚úì Load base model: AnimeXL_v3 (6.5GB)
‚úì Load VAE: sdxl_vae (0.2GB)
‚úì Load LoRAs: anime_girl_v2, pink_hair_style
‚úì Apply memory optimization: BALANCED
‚úì Generate 1024x1024 image
‚úì Cleanup: offload to CPU, clear cache
```

Result: Perfect anime girl with pink hair! üé®

## Statistics and Monitoring

```python
stats = builder.get_stats()
```

Returns:
```python
{
    'device': 'cuda',
    'ollama_enabled': True,
    'orchestrator': {
        'total_models': 3735,
        'by_type': {
            'lora': 3678,
            'checkpoint': 45,
            'vae': 12
        }
    },
    'resources': {
        'gpu_memory_gb': 24.0,
        'ram_gb': 32.5
    }
}
```

## Configuration

All configuration is **optional** and controlled by USER:

```python
# Option 1: Auto-detect (easiest)
builder = IntelligentPipelineBuilder.from_comfyui_auto()

# Option 2: Custom search paths
builder = IntelligentPipelineBuilder.from_comfyui_auto(
    search_paths=["/opt/comfyui", "/home/user/comfyui"]
)

# Option 3: Explicit paths
builder = IntelligentPipelineBuilder.from_paths({
    "checkpoint": ["/models/checkpoints"],
    "lora": ["/models/loras"],
    "vae": ["/models/vae"]
})

# Option 4: Full control with ModelPathConfig
from ml_lib.diffusion.config import ModelPathConfig

config = ModelPathConfig(
    checkpoint_paths=[Path("/my/checkpoints")],
    lora_paths=[Path("/my/loras")],
    vae_paths=[Path("/my/vaes")]
)

builder = IntelligentPipelineBuilder(
    model_config=config,
    enable_ollama=True,
    device="cuda"
)
```

**No environment variables. No hardcoded paths. Pure OOP.**

## Privacy and Security

### MetadataFetcher
- ‚úÖ Anonymous requests (no API keys)
- ‚úÖ No user tracking
- ‚úÖ Rate limiting (respectful to APIs)
- ‚úÖ Local caching (minimize requests)
- ‚úÖ Configurable cache TTL

### Ollama
- ‚úÖ Runs locally (if you have Ollama installed)
- ‚úÖ Fallback to keyword analysis if unavailable
- ‚úÖ Configurable endpoint

### Data
- ‚úÖ All processing local
- ‚úÖ No telemetry
- ‚úÖ No data sent to external services (except public API lookups)

## Performance

### Memory Optimization
- Automatic based on available GPU
- Tested with 8GB, 12GB, 24GB GPUs
- CPU fallback for no GPU

### Speed
- Metadata cached locally (fast lookups)
- Model selection < 1s
- Generation speed depends on hardware + quality

### Scalability
- Tested with 3,678 LoRAs
- Efficient indexing
- Lazy loading

## Future Enhancements

Planned:
1. ‚úÖ ControlNet integration
2. ‚úÖ IP-Adapter integration
3. ‚úÖ Style transfer
4. ‚è≥ Batch processing optimization
5. ‚è≥ Distributed generation
6. ‚è≥ Fine-tuning support
7. ‚è≥ Training pipeline

## Summary

**For Users**:
```python
# This is ALL you need to know:
builder = IntelligentPipelineBuilder.from_comfyui_auto(enable_ollama=True)
image = builder.generate("your prompt", style="your style", quality="high")
```

**System handles**:
- Model discovery and indexing
- Metadata fetching and parsing
- Semantic prompt analysis
- Model selection and scoring
- Component compatibility
- Memory optimization
- Parameter tuning
- Resource management
- Cleanup

**Market value**: Fastest, smartest, easiest diffusion library. User provides prompt, gets perfect image. Zero technical knowledge required.
