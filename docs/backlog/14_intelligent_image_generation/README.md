# √âpica 14: Intelligent Image Generation

**Estado:** üìã PLANIFICADO
**Prioridad:** ‚ö° ALTA
**Estimaci√≥n Total:** 116 horas (~14-15 d√≠as)

---

## üìã Visi√≥n General

Sistema inteligente de generaci√≥n de im√°genes que integra HuggingFace Hub y CivitAI para seleccionar autom√°ticamente modelos, LoRAs y par√°metros √≥ptimos bas√°ndose en el an√°lisis sem√°ntico del prompt del usuario, con gesti√≥n eficiente de memoria para trabajar en hardware limitado.

### Objetivos de Negocio

1. **Democratizar generaci√≥n de IA**: Permitir a usuarios sin experiencia t√©cnica generar im√°genes de alta calidad
2. **Optimizar recursos**: Usar memoria eficientemente para ejecutar en GPUs consumer
3. **Automatizaci√≥n inteligente**: Reducir trial-and-error mediante selecci√≥n autom√°tica de componentes
4. **Aprendizaje continuo**: Mejorar recomendaciones con feedback del usuario

---

## üìä User Stories

### ‚úÖ US 14.1: Integraci√≥n con Model Hubs
**Estimaci√≥n:** 32 horas | **Prioridad:** ‚ö° CR√çTICA

Integraci√≥n completa con HuggingFace Hub y CivitAI para b√∫squeda, descarga y gesti√≥n de modelos y LoRAs.

**Entregables:**
- `HuggingFaceHubService`: Cliente para HF Hub
- `CivitAIService`: Cliente para CivitAI API
- `ModelRegistry`: Registro unificado de modelos
- Cache local inteligente con verificaci√≥n de integridad

[üìÑ Ver detalle completo](US_14.1_model_hub_integration.md)

---

### ‚úÖ US 14.2: Sistema Inteligente de Prompting
**Estimaci√≥n:** 40 horas | **Prioridad:** ‚ö° CR√çTICA

An√°lisis sem√°ntico de prompts usando Ollama (via m√≥dulo `ml_lib.llm`) para recomendar LoRAs y optimizar par√°metros autom√°ticamente.

**Entregables:**
- `PromptAnalyzer`: An√°lisis sem√°ntico con Ollama
- `LoRARecommender`: Recomendaci√≥n basada en embeddings
- `ParameterOptimizer`: Optimizaci√≥n multi-objetivo
- `LearningEngine`: Sistema de feedback y aprendizaje continuo

**Componentes clave:**
- Extracci√≥n de conceptos (estilo, sujeto, escena, calidad)
- Detecci√≥n de intenci√≥n art√≠stica
- Scoring multi-factor para LoRAs
- Balanceo autom√°tico de pesos (alpha)

[üìÑ Ver detalle completo](US_14.2_intelligent_prompting_system.md)

---

### ‚úÖ US 14.3: Gesti√≥n Eficiente de Memoria
**Estimaci√≥n:** 24 horas | **Prioridad:** ‚ö° CR√çTICA

Gesti√≥n din√°mica de memoria con offloading autom√°tico a CPU/disco para trabajar con modelos grandes en GPUs peque√±as (<8GB VRAM).

**Entregables:**
- `MemoryManager`: Detecci√≥n de recursos y tracking
- `ModelPool`: Pool con LRU eviction
- `ModelOffloader`: Offload autom√°tico CPU/GPU
- `QuantizationEngine`: Quantizaci√≥n autom√°tica (fp16, int8)
- Integraci√≥n con xformers, Flash Attention

**Estrategias:**
- AUTO: Decisi√≥n autom√°tica seg√∫n VRAM
- SEQUENTIAL: Carga componentes bajo demanda
- CPU_OFFLOAD: UNet en GPU, resto en CPU
- FULL_GPU: Todo en GPU (requiere 16GB+)

[üìÑ Ver detalle completo](US_14.3_efficient_memory_management.md)

---

### ‚úÖ US 14.4: Pipeline Integration
**Estimaci√≥n:** 20 horas | **Prioridad:** ‚ö° CR√çTICA

Pipeline unificado que integra todos los subsistemas en una API coherente con modos AUTO, ASSISTED y MANUAL.

**Entregables:**
- `IntelligentGenerationPipeline`: Pipeline principal
- Workflow completo: An√°lisis ‚Üí Recomendaci√≥n ‚Üí Optimizaci√≥n ‚Üí Generaci√≥n
- Batch processing con estrategias de variaci√≥n
- Sistema de explicaciones de decisiones
- Feedback loop para aprendizaje

**Modos de operaci√≥n:**
- **AUTO**: Decisiones completamente autom√°ticas
- **ASSISTED**: Muestra recomendaciones, usuario aprueba/modifica
- **MANUAL**: Control total del usuario

[üìÑ Ver detalle completo](US_14.4_pipeline_integration.md)

---

## üèóÔ∏è Arquitectura General

```mermaid
graph TB
    subgraph "User Layer"
        User[Usuario]
        Prompt[Prompt Text]
    end

    subgraph "Intelligent Pipeline - US 14.4"
        PipelineAPI[Pipeline API]
    end

    subgraph "Intelligence Layer - US 14.2"
        Ollama[Ollama LLM<br/>ml_lib.llm]
        Analyzer[Prompt Analyzer]
        Recommender[LoRA Recommender]
        Optimizer[Parameter Optimizer]
    end

    subgraph "Resource Layer - US 14.1 & 14.3"
        HFHub[HuggingFace Hub]
        CivitAI[CivitAI API]
        Registry[Model Registry]
        MemoryMgr[Memory Manager]
        ModelPool[Model Pool]
    end

    subgraph "Execution Layer"
        DiffusionPipeline[Diffusion Pipeline]
        GPU[GPU / VRAM]
        CPU[CPU / RAM]
    end

    subgraph "Learning Layer"
        Feedback[Feedback System]
        LearningDB[(Learning Database)]
    end

    User --> Prompt
    Prompt --> PipelineAPI

    PipelineAPI --> Analyzer
    Analyzer --> Ollama
    Analyzer --> Recommender
    Analyzer --> Optimizer

    Recommender --> Registry
    Registry --> HFHub
    Registry --> CivitAI

    Optimizer --> MemoryMgr
    MemoryMgr --> ModelPool

    ModelPool --> DiffusionPipeline
    DiffusionPipeline --> GPU
    DiffusionPipeline --> CPU

    DiffusionPipeline --> Image[Generated Image]
    Image --> User

    User --> Feedback
    Feedback --> LearningDB
    LearningDB -.learns.-> Recommender
    LearningDB -.learns.-> Optimizer
```

---

## üóÇÔ∏è Estructura de C√≥digo

```
ml_lib/diffusion/
‚îú‚îÄ‚îÄ intelligent/                      # Nuevo subm√≥dulo
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ hub_integration/              # US 14.1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ huggingface_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ civitai_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_registry.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ entities/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ model_metadata.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ download_result.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ model_filter.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ prompting/                    # US 14.2
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_analyzer.py        # Usa ml_lib.llm.OllamaProvider
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lora_recommender.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parameter_optimizer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ learning_engine.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ entities/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ prompt_analysis.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ lora_recommendation.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ optimized_parameters.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ intent.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ memory/                       # US 14.3
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory_manager.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_pool.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_offloader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quantization_engine.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ entities/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ system_resources.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ offload_config.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ loaded_model.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ pipeline/                     # US 14.4
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ intelligent_pipeline.py
‚îÇ       ‚îú‚îÄ‚îÄ batch_processor.py
‚îÇ       ‚îú‚îÄ‚îÄ decision_explainer.py
‚îÇ       ‚îú‚îÄ‚îÄ feedback_collector.py
‚îÇ       ‚îî‚îÄ‚îÄ entities/
‚îÇ           ‚îú‚îÄ‚îÄ pipeline_config.py
‚îÇ           ‚îú‚îÄ‚îÄ generation_result.py
‚îÇ           ‚îú‚îÄ‚îÄ generation_metadata.py
‚îÇ           ‚îî‚îÄ‚îÄ batch_config.py
‚îÇ
‚îú‚îÄ‚îÄ services/                         # Servicios existentes
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_service.py
‚îÇ   ‚îú‚îÄ‚îÄ lora_service.py
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ docs/                             # Documentaci√≥n existente
    ‚îú‚îÄ‚îÄ readme.md
    ‚îú‚îÄ‚îÄ intelligent_prompting_system.md
    ‚îî‚îÄ‚îÄ ...
```

---

## üìÖ Roadmap y Priorizaci√≥n

### Sprint 1: Foundations (Semana 1-2)
**Duraci√≥n:** 2 semanas | **Horas:** 56h

- [x] US 14.1: Model Hub Integration (32h)
  - HuggingFace Hub service
  - CivitAI service
  - Model Registry
  - Cache management

- [ ] US 14.3: Memory Management (24h)
  - Resource detection
  - Model Pool con LRU
  - Basic offloading

**Entregable:** Sistema capaz de buscar, descargar y cachear modelos con gesti√≥n de memoria

---

### Sprint 2: Intelligence (Semana 3-4)
**Duraci√≥n:** 2 semanas | **Horas:** 40h

- [ ] US 14.2: Intelligent Prompting (40h)
  - Prompt Analyzer con Ollama
  - LoRA Recommender
  - Parameter Optimizer
  - Learning Engine

**Entregable:** Sistema que analiza prompts y recomienda LoRAs/par√°metros

---

### Sprint 3: Integration (Semana 5)
**Duraci√≥n:** 1 semana | **Horas:** 20h

- [ ] US 14.4: Pipeline Integration (20h)
  - Intelligent Pipeline
  - Batch processing
  - Decision explainer
  - Feedback loop

**Entregable:** Pipeline end-to-end completamente funcional

---

## üß™ Testing Strategy

### Unit Tests
- Cada componente con >80% coverage
- Mocks para APIs externas (HF, CivitAI, Ollama)
- Tests de edge cases (OOM, network errors)

### Integration Tests
- Workflow completo con modelos peque√±os
- Tests en diferentes configuraciones de VRAM
- Verificaci√≥n de cache persistence

### Performance Tests
- Benchmarks de tiempos de generaci√≥n
- Memory usage profiling
- Throughput en batch processing

### User Acceptance Tests
- Generaci√≥n con prompts reales
- Comparaci√≥n con generaci√≥n manual experta
- Feedback qualitativo de usuarios

---

## üìä M√©tricas de √âxito

### M√©tricas T√©cnicas
- **Tiempo de setup**: <30s para cargar pipeline
- **Uso de memoria**: Generaci√≥n SDXL con <8GB VRAM
- **Calidad de recomendaciones**: >70% de LoRAs recomendados son relevantes
- **Accuracy de par√°metros**: Par√°metros √≥ptimos en >80% de casos

### M√©tricas de Usuario
- **Satisfacci√≥n**: Rating promedio >4/5 en generaciones AUTO
- **Reducci√≥n de iteraciones**: <2 intentos promedio vs >5 en modo manual
- **Adopci√≥n**: >60% de usuarios prefieren modo AUTO vs MANUAL

---

## üîó Dependencias

### Internas
- `ml_lib.llm` - Para an√°lisis sem√°ntico con Ollama ‚úÖ Existe
- `ml_lib.diffusion.services` - Servicios base de diffusion ‚úÖ Existe
- `ml_lib.core` - Interfaces y validaci√≥n ‚úÖ Existe

### Externas
```requirements
torch>=2.0.0
diffusers>=0.25.0
transformers>=4.36.0
huggingface-hub>=0.20.0
sentence-transformers>=2.2.0
safetensors>=0.4.0
requests>=2.31.0
aiohttp>=3.9.0
psutil>=5.9.0
bitsandbytes>=0.41.0  # INT8 quantization
xformers>=0.0.20  # Optional, memory-efficient attention
Pillow>=10.0.0
tqdm>=4.66.0
```

---

## üìù Notas de Implementaci√≥n

### Integraci√≥n con Ollama

El an√°lisis sem√°ntico de prompts se realizar√° mediante el m√≥dulo `ml_lib.llm` existente:

```python
from ml_lib.llm.providers import OllamaProvider
from ml_lib.llm.clients import LLMClient

# Configurar Ollama
provider = OllamaProvider(
    base_url="http://localhost:11434",
    model="llama2:7b"  # o "mistral", "codellama", etc.
)

client = LLMClient(provider=provider)

# Analizar prompt
response = client.generate(
    prompt=f"Analyze this image generation prompt and extract style, subject, mood: '{user_prompt}'"
)
```

### Consideraciones de Memoria

Para VRAM limitado (<8GB), el sistema autom√°ticamente:
1. Detecta VRAM disponible
2. Selecciona estrategia de offloading apropiada
3. Cuantiza modelos a fp16 o int8
4. Usa Sequential loading si es necesario

### Cache y Persistencia

- Modelos descargados se cachean en `~/.cache/ml_lib/models/`
- Metadata en SQLite: `~/.ml_lib/models.db`
- Feedback history: `~/.ml_lib/feedback.db`
- Learning data se persiste entre sesiones

---

## üöÄ Getting Started (Post-implementaci√≥n)

### Instalaci√≥n

```bash
pip install ml-lib[diffusion]  # Incluye dependencias de diffusion
```

### Uso B√°sico

```python
from ml_lib.diffusion.intelligent import IntelligentGenerationPipeline

# Modo AUTO: Todo autom√°tico
pipeline = IntelligentGenerationPipeline()

result = pipeline.generate(
    prompt="anime girl with magical powers in Victorian mansion"
)

result.image.save("output.png")
print(result.explanation.summary)
# "Selected anime_style_lora (Œ±=0.8), detail_enhancer (Œ±=0.5) |
#  Parameters: 35 steps, CFG 7.5, 1024√ó1024 | Complexity: moderate"
```

### Uso Avanzado

```python
from ml_lib.diffusion.intelligent import (
    IntelligentGenerationPipeline,
    PipelineConfig,
    GenerationConstraints,
    Priority
)

# Configuraci√≥n custom
config = PipelineConfig(
    base_model="stabilityai/sdxl-base-1.0",
    constraints=GenerationConstraints(
        max_vram_gb=8.0,
        priority=Priority.QUALITY
    )
)

pipeline = IntelligentGenerationPipeline(config=config)

# Generaci√≥n batch
results = pipeline.generate_batch(
    prompt="anime character, different poses",
    batch_config=BatchConfig(num_images=4)
)
```

---

**√öltima actualizaci√≥n:** 2025-10-09
**Responsable:** Equipo ML
**Revisores:** TBD
