
# User Story 10.1: Procesamiento Fuera de Memoria (Out-of-Core)

**Como científico de datos,** necesito poder entrenar modelos y procesar conjuntos de datos que son más grandes que la memoria RAM de mi máquina, utilizando algoritmos que operen en lotes (batches) o por streaming.

## Tareas:

- **Task 10.1.1:** Crear una `DataLoaderInterface` y un `StreamingService` que pueda leer datos en lotes (chunks) desde archivos (CSV, etc.).
- **Task 10.1.2:** Adaptar la `EstimatorInterface` para que incluya un método `partial_fit` para el aprendizaje incremental/online.
- **Task 10.1.3:** Implementar versiones incrementales de algoritmos clave, como `IncrementalPCA` y `SGDClassifier`/`SGDRegressor` que usen `partial_fit`.
- **Task 10.1.4:** Investigar el uso de `memory mapping` para acceder a porciones de archivos en disco como si estuvieran en memoria.
